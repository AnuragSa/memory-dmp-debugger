# OpenAI Configuration
#OPENAI_API_KEY=your-openai-api-key-here
#OPENAI_MODEL=gpt-4-turbo-preview

# Anthropic Configuration (alternative)
# ANTHROPIC_API_KEY=your-anthropic-api-key-here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Azure Configuration - works for both Azure OpenAI and Azure AI Foundry

AZURE_OPENAI_API_KEY=[YOUR_AZURE_OPENAI_API_KEY_HERE]
AZURE_OPENAI_ENDPOINT=[YOUR_AZURE_OPENAI_ENDPOINT_HERE] # e.g. "https://<your-instance>.services.ai.azure.com/anthropic/"
AZURE_OPENAI_DEPLOYMENT=[YOUR_AZURE_OPENAI_DEPLOYMENT_HERE] # e.g. "claude-sonnet-4-5"
AZURE_OPENAI_API_VERSION=[YOUR_AZURE_OPENAI_API_VERSION_HERE] # e.g. "2024-05-01-preview"


# LLM Provider (openai, anthropic, azure, ollama)
LLM_PROVIDER=azure

# Local LLM Configuration (Ollama). See SETUP.md for details.
# Enable local LLM for cost/speed optimization
USE_LOCAL_LLM=true
LOCAL_LLM_BASE_URL=http://localhost:11434
LOCAL_LLM_MODEL=qwen2.5-coder:7b
LOCAL_LLM_TIMEOUT=120
#LOCAL_LLM_CONTEXT_SIZE=32768

# Tiered LLM Strategy
# Route simple tasks to local LLM, complex tasks to cloud LLM
USE_TIERED_LLM=true
CLOUD_LLM_PROVIDER=azure

# WinDbg/CDB Configuration
CDB_PATH=C:\\Program Files (x86)\\Windows Kits\\10\\Debuggers\\x64\\cdb.exe
WINDBG_PATH=C:\\Program Files (x86)\\Windows Kits\\10\\Debuggers\\x64\\windbg.exe

# Symbol Server Configuration
# Format: SRV*local_cache_dir*https://symbol_server
# The cache directory will be created automatically if it doesn't exist
SYMBOL_PATH=SRV*c:\\symbols*https://msdl.microsoft.com/download/symbols

# Debugger Command Timeout (in seconds)
# Default: 1800 seconds (30 minutes) for slow commands like !gcroot -all, !dumpheap
COMMAND_TIMEOUT=1800

# SOS Extension Configuration (optional)
# Specify custom SOS.dll path if analyzing dumps from different .NET runtime versions
# Leave commented to auto-detect SOS from the dump's runtime
# Example for .NET 6.0: C:\\Program Files\\dotnet\\shared\\Microsoft.NETCore.App\\6.0.x\\sos.dll
# Example for .NET Framework 4.8: C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\sos.dll
#SOS_DLL_PATH=C:\\path\\to\\sos.dll

# DAC (Data Access Component) Configuration (optional but CRITICAL)
# Specify custom mscordacwks.dll path for exact runtime version matching
# MUST match the EXACT build version of .NET runtime in the dump
# Leave commented to auto-download from symbol server (recommended)
# Example for .NET 6.0: C:\\Program Files\\dotnet\\shared\\Microsoft.NETCore.App\\6.0.25\\mscordacwks.dll
# Example for .NET Core 3.1: C:\\Program Files\\dotnet\\shared\\Microsoft.NETCore.App\\3.1.32\\mscordaccore.dll
# Example for .NET Framework 4.8: C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\mscordacwks.dll
#MSCORDACWKS_PATH=C:\\path\\to\\mscordacwks.dll

# Logging
LOG_LEVEL=INFO


# Data model commands (set to false to force classic commands only)
ENABLE_DATA_MODEL_COMMANDS=true

# Workflow Configuration
# LangGraph recursion limit - max workflow iterations (each chat question counts as 1)
# Increase if you plan to ask many follow-up questions in interactive mode
GRAPH_RECURSION_LIMIT=200

# Evidence Management & Semantic Search
# Use embeddings for semantic search in interactive mode (requires Azure OpenAI or OpenAI)
USE_EMBEDDINGS=true

# Embeddings Provider (openai or azure)
EMBEDDINGS_PROVIDER=azure

# Azure OpenAI Embeddings Configuration (if EMBEDDINGS_PROVIDER=azure)
# Use separate deployment for embeddings or reuse main endpoint
AZURE_EMBEDDINGS_DEPLOYMENT=text-embedding-3-small
# Optional: use different endpoint for embeddings (defaults to AZURE_OPENAI_ENDPOINT)
AZURE_EMBEDDINGS_ENDPOINT=[YOUR_AZURE_EMBEDDINGS_ENDPOINT_HERE] # e.g. "https://<your-instance>.services.ai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15"  
# Optional: use different API key for embeddings (defaults to AZURE_OPENAI_API_KEY)
AZURE_EMBEDDINGS_API_KEY=[YOUR_EMBEDDING_MODEL_API_KEY_HERE]

# OpenAI Embeddings Configuration (if EMBEDDINGS_PROVIDER=openai)
#EMBEDDINGS_MODEL=text-embedding-3-small

# Storage thresholds
EVIDENCE_STORAGE_THRESHOLD=250000  # Store outputs larger than 250KB externally (optimized for Claude 4.5)
EVIDENCE_CHUNK_SIZE=250000         # Chunk size for LLM analysis (250KB for Claude Sonnet 4.5)
CHUNK_ANALYSIS_DELAY=2.0           # Delay in seconds between chunk analyses to avoid rate limits
